{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import load_dataset\n",
    "import lion_pytorch\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве основы я выбрал новую модель Florence2-base от Microsoft, и это не случайно. Есть несколько причин для этого выбора:\n",
    "\n",
    "1. Малое количество  параметров: Florence2 имеет всего 270 миллионов параметров, что облегчает ее дообучение. Благодаря этому процесс вывода и квантования в этой модели будет проще, чем в более массивных вариантах.\n",
    "   \n",
    "2. Обильные данные: Florence2 обучалась на данных, включающих 126 миллионов изображений в различных задачах, таких как классификация, сегментация и обнаружение объектов. Это гарантирует более высокую эффективность модели в задаче CQA.\n",
    "\n",
    "Сама структура модели немного отличается от стандартной VLM: в то время как большинство VLM состоят из энкодера и декодера, Florence2 включает в себя три ключевые компоненты - Визуальный Энкодер, Энкодер Вопросов и Декодер. \n",
    "\n",
    "Для получения эмбеддингов изображений используется DaViT. Эмбеддинги из визуального энкодера затем проходят через энкодер вопросов, где комбинируются с токенами. Затем эмбеддинги вопросов поступают в декодер, который завершает моделирование и генерацию текста\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Florence2ForConditionalGeneration is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "270803968"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", attn_implementation=\"flash_attention_2\", trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/Florence-2-base-ft\", trust_remote_code=True)\n",
    "path = '/home/oleg/models/checkpoint-1000/'\n",
    "model = AutoModelForCausalLM.from_pretrained(path, config=model.config, attn_implementation=\"flash_attention_2\", use_safetensors=True, trust_remote_code=True).cuda()\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выбор датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для процесса настройки я решил использовать датасет ChartQA от команды HuggingFaceM, и в этом есть несколько причин:\n",
    "\n",
    "1. Объемный датасет: Содержащий 27 тысяч элементов, этот датасет обеспечивает достаточно данных для успешного дообучения модели на аналогичную задачу.\n",
    "   \n",
    "2. Предыдущий опыт модели: Учитывая то, что модель была обучена на задачах детекции и сегментации, это позволит ей более точно определять содержимое изображений, что, в свою очередь, повысит точность ответов модели.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(data):\n",
    "    image = data['image'].convert(\"RGB\")\n",
    "    query = data['query']\n",
    "    label = data['label']\n",
    "\n",
    "    out = processor(text=query, images=image, return_tensors=\"pt\", padding=False)\n",
    "    label = processor.tokenizer(text=label, return_tensors='pt', padding=False)['input_ids']\n",
    "\n",
    "    data['input_ids'] = out['input_ids'].squeeze(0)\n",
    "    data['pixel_values'] = out['pixel_values'].squeeze(0)\n",
    "    data['labels'] = label.squeeze(0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"HuggingFaceM4/ChartQA\", streaming=True)['train']\n",
    "ds = ds.map(fn, batched=False, remove_columns=['image', 'human_or_machine', 'query', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сохранения ранее полученных знаний модели я принял решение заморозить визуальный энкодер. \n",
    "\n",
    "1. В качестве оптимизатора был выбран Lion с параметрами: learning rate = 2e-5,  betas = (0.9, 0.99) и weight decay = 2.5e-2.\n",
    "\n",
    "2. Был выбран размер пакета  равный 1024 и выполнено 100 шагов, что приблизительно эквивалентно 10 эпохам обучения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.vision_tower.parameters():\n",
    "  param.is_trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.steps = 0\n",
    "        self.loss = 0\n",
    "        self.epoch = 0.0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        clear_output(wait=True)\n",
    "        if state.log_history:\n",
    "            print(f\"Batch {state.global_step}: Loss = {state.log_history[-1]}\")\n",
    "            self.steps +=1\n",
    "            self.loss += state.log_history[-1]['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = lion_pytorch.Lion(model.parameters(),lr=2e-5, betas=(0.9, 0.99), weight_decay=2.5e-2, use_triton=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/oleg/models/\",  \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1024,  \n",
    "    max_steps=100,\n",
    "    gradient_checkpointing=False,\n",
    "    fp16=True,\n",
    "    warmup_steps=0,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_steps= 1,\n",
    "    do_eval = False,\n",
    "    logging_steps=1,    \n",
    "    max_grad_norm =0.75,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=1e-2,\n",
    "    adam_beta1=0.91,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-8,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    remove_unused_columns=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=ds,\n",
    "    optimizers=(optimizer,None),\n",
    "    callbacks=[BatchLossCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение моделей до и после обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества я решил использовать метрику WER (Word Error Rate). Хотя обычно WER применяется в задачах распознавания речи, в данном датасете ответы состоят либо из одного слова, либо из одного числа. Поэтому WER будет работать аналогично точности и может показать себя более эффективно, чем другие метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/home/oleg/models/checkpoint-100/'\n",
    "orig_model =  AutoModelForCausalLM.from_pretrained(\"microsoft/Florence-2-base-ft\", attn_implementation=\"flash_attention_2\", trust_remote_code=True).eval().cuda()\n",
    "tuned_model = AutoModelForCausalLM.from_pretrained(path, config=model.config, use_safetensors=True, trust_remote_code=True).eval().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"HuggingFaceM4/ChartQA\", streaming=True)['test']\n",
    "ds = ds.map(fn, batched=False, remove_columns=['image', 'human_or_machine', 'query', 'label'])\n",
    "dataloader = DataLoader(ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499 \n"
     ]
    }
   ],
   "source": [
    "pred_tuned = []\n",
    "pred_orig = []\n",
    "references = []\n",
    "for idx, batch in enumerate(dataloader):\n",
    "\n",
    "    input_ids = batch['input_ids'].cuda()\n",
    "    pixel_values = batch['pixel_values'].cuda()\n",
    "    labels = batch['labels'].cuda()\n",
    "    l = labels.shape[1]\n",
    "    with torch.no_grad():\n",
    "        out_tuned = tuned_model.generate(input_ids=input_ids, pixel_values=pixel_values, max_new_tokens=l)\n",
    "        out_orig = orig_model.generate(input_ids=input_ids, pixel_values=pixel_values, max_new_tokens=l)\n",
    "    s_tuned = processor.tokenizer.decode(out_tuned.cpu()[0, :], skip_special_tokens=True)\n",
    "    s_orig = processor.tokenizer.decode(out_orig.cpu()[0, :], skip_special_tokens=True)\n",
    "    s = processor.tokenizer.decode(labels.cpu()[0, :], skip_special_tokens=True)\n",
    "    pred_tuned.append(s_tuned)\n",
    "    pred_orig.append(s_orig)\n",
    "    references.append(s)\n",
    "    clear_output(True)\n",
    "    print(f'{ idx} ')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9884816753926702\n",
      "0.09598603839441536\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "\n",
    "orig_wer_score = wer.compute(predictions=pred_orig, references=references)\n",
    "tuned_wer_score = wer.compute(predictions=pred_tuned, references=references)\n",
    "print(orig_wer_score)\n",
    "print(tuned_wer_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pred_orig, pred_tuned, references)), columns=['orig', 'tuned', 'references'])\n",
    "df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе проведения исследования удалось значительно улучшить показатели модели с 0.98 до 0.1. Полученные результаты свидетельствуют о том, что во многих задачах обработки визуальных вопросов и ответов (VQA) использование огромных языковых моделей не является обязательным. Достаточно использовать меньшие модели с хорошо отфильтрованными данными."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
